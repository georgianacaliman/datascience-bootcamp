{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9d86cf74-7cf9-4ea9-bc5d-159e35a16513",
   "metadata": {},
   "source": [
    "## <ins>Data Science Bootcamp - 3th Day- Statistics and ML</ins>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "17f39c02-640f-4566-b9f3-781798007d61",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import r2_score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06522d70-c852-4524-9a93-f19a12e69bac",
   "metadata": {},
   "source": [
    "# Exercises"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a83a0740-24f5-4271-844c-e9cfa85aa5b2",
   "metadata": {},
   "source": [
    "# Data types\n",
    "A: Consider the following data and figure out what category of data they are.<br>\n",
    "nominal, ordinal, discrete, continous\n",
    "\n",
    "What is your ethnicity?\n",
    "â€“ Central Asian\n",
    "- Indonesian\n",
    "- West Asian\n",
    "- Japanese\n",
    "- Indian\n",
    "\n",
    "B: The revenue from sales of a company:\n",
    "- $2.8b\n",
    "\n",
    "C: The number of sales of a company:\n",
    "- Product A: 1753\n",
    "- Product B: 104357\n",
    "- Product C: 3\n",
    "\n",
    "D: Grades in school:\n",
    "- 1\n",
    "- 2\n",
    "- 3\n",
    "- 4\n",
    "- 5\n",
    "- 6\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c74fb109-540b-4007-9224-37321ba62fe5",
   "metadata": {},
   "source": [
    "- A:\n",
    "- B:\n",
    "- C:\n",
    "- D:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ca206fa-07bd-4db6-b518-0a571200b268",
   "metadata": {},
   "source": [
    "# Probabilities\n",
    "What of the following are probailites?\n",
    "- A: P(X) = -0.1\n",
    "- B: P(X) = 1.1\n",
    "- C: P(X) = 1.0\n",
    "- D: P(X) = 0.4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cbba5f0-d20e-4bd4-bd60-e29cca89d3c5",
   "metadata": {},
   "source": [
    "# Confidence intervall:\n",
    "We want to calculate the confidence intervall of a given data set.<br> The formula for the CI is given by:<br>\n",
    "$\\bar{X}$ is the mean of data set.<br>\n",
    "$u$  is the quantile value.<br>\n",
    "$\\sigma$ is the standartdeviation.<br>\n",
    "$n$ is the number of oberservations.<br>\n",
    "$[upper, lower] = \\bar{X}\\pm u\\cdot\\frac{\\sigma}{\\sqrt{n}}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bacba79b-9418-4dd7-9812-c8baab64df24",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    }
   ],
   "source": [
    "data = [45, 55, 67, 45, 68, 79, 98, 87, 84, 82] #given dataset\n",
    "n = len(data)\n",
    "#uncomment the next lime and caculate the mean the the dataset\n",
    "#x = sum(None)/len(None) \n",
    "print(None) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b4adadc3-9efe-48cc-9e44-aa341b43962d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17.23948955160796\n"
     ]
    }
   ],
   "source": [
    "sigma = np.std(data) #calculates the standartdeviation of the dataset\n",
    "print(sigma)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "318e11bd-5ac5-47cd-b342-2dab5076915b",
   "metadata": {},
   "source": [
    "From the quantile table we get the u-value of 2.821."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bb34377c-d730-4ab0-8542-d24281324758",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None None\n"
     ]
    }
   ],
   "source": [
    "u = 2.821\n",
    "#write here the formula for the upper and lower bound of the confidence intervall\n",
    "#note the root of a variable is math.sqrt(x)\n",
    "upper = None\n",
    "lower = None\n",
    "print(lower, upper)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32039ff1-82bb-48a7-9c2e-5d24be3a179a",
   "metadata": {},
   "source": [
    "# Choose which kind of ML might the most fitting for the problem:\n",
    "- Predict the future returns of a company.\n",
    "- Data, in which we want to find commonalities, group them together into categories to better advertise to them.\n",
    "- We want to train a robot to do complex maneuvers, but we dont have a lot of data to train the robot on."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a66f594-0349-4412-a1fb-9000d1d73080",
   "metadata": {},
   "source": [
    "# Linear regression\n",
    "In order to get used to the simplest form of ML, linear regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1ff5ee35-7dac-4c4f-b6e5-3d0efe5b28ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(42)\n",
    "#Randomly creates data\n",
    "n_samples, n_features = 200, 50\n",
    "X = np.random.randn(n_samples, n_features)\n",
    "true_coef = 3 * np.random.randn(n_features)\n",
    "# Threshold coefficients to render them non-negative\n",
    "true_coef[true_coef < 0] = 0\n",
    "y = np.dot(X, true_coef)\n",
    "\n",
    "# Add some noise\n",
    "y += 5 * np.random.normal(size=(n_samples,))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b3ab709a-cb61-4e3f-9a2b-7b1a22c47c2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "change the \"Nones\" below to split the data into two set, note the dataset are X and y\n",
    "X_train, X_test, y_train, y_test = train_test_split(None, None, test_size=0.5) #split the dataset into two, by half/0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "901cb1e1-71c7-4115-a79c-af41b4f73f1f",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "This LinearRegression estimator requires y to be passed, but the target y is None.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Input \u001b[1;32mIn [8]\u001b[0m, in \u001b[0;36m<cell line: 5>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      3\u001b[0m reg_nnls \u001b[38;5;241m=\u001b[39m LinearRegression(positive\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m      4\u001b[0m \u001b[38;5;66;03m#add the above created training dataset to the fit\u001b[39;00m\n\u001b[1;32m----> 5\u001b[0m y_pred_nnls \u001b[38;5;241m=\u001b[39m \u001b[43mreg_nnls\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mpredict(X_test)\n\u001b[0;32m      6\u001b[0m r2_score_nnls \u001b[38;5;241m=\u001b[39m r2_score(y_test, y_pred_nnls)\u001b[38;5;66;03m#gets the R^2 of the modell\u001b[39;00m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNNLS R2 score\u001b[39m\u001b[38;5;124m\"\u001b[39m, r2_score_nnls)\n",
      "File \u001b[1;32m~\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:662\u001b[0m, in \u001b[0;36mLinearRegression.fit\u001b[1;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[0;32m    658\u001b[0m n_jobs_ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_jobs\n\u001b[0;32m    660\u001b[0m accept_sparse \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpositive \u001b[38;5;28;01melse\u001b[39;00m [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcsr\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcsc\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcoo\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m--> 662\u001b[0m X, y \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_validate_data\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    663\u001b[0m \u001b[43m    \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maccept_sparse\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maccept_sparse\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_numeric\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmulti_output\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\n\u001b[0;32m    664\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    666\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m sample_weight \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    667\u001b[0m     sample_weight \u001b[38;5;241m=\u001b[39m _check_sample_weight(sample_weight, X, dtype\u001b[38;5;241m=\u001b[39mX\u001b[38;5;241m.\u001b[39mdtype)\n",
      "File \u001b[1;32m~\\Anaconda3\\lib\\site-packages\\sklearn\\base.py:555\u001b[0m, in \u001b[0;36mBaseEstimator._validate_data\u001b[1;34m(self, X, y, reset, validate_separately, **check_params)\u001b[0m\n\u001b[0;32m    552\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_feature_names(X, reset\u001b[38;5;241m=\u001b[39mreset)\n\u001b[0;32m    554\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m y \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_tags()[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrequires_y\u001b[39m\u001b[38;5;124m\"\u001b[39m]:\n\u001b[1;32m--> 555\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    556\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThis \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m estimator \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    557\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrequires y to be passed, but the target y is None.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    558\u001b[0m     )\n\u001b[0;32m    560\u001b[0m no_val_X \u001b[38;5;241m=\u001b[39m \u001b[38;5;28misinstance\u001b[39m(X, \u001b[38;5;28mstr\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m X \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mno_validation\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    561\u001b[0m no_val_y \u001b[38;5;241m=\u001b[39m y \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(y, \u001b[38;5;28mstr\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m y \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mno_validation\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "\u001b[1;31mValueError\u001b[0m: This LinearRegression estimator requires y to be passed, but the target y is None."
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "reg_nnls = LinearRegression(positive=True)\n",
    "#add the above created training dataset to the fit\n",
    "y_pred_nnls = reg_nnls.fit(None, None).predict(X_test)\n",
    "r2_score_nnls = r2_score(y_test, y_pred_nnls)#gets the R^2 of the modell\n",
    "print(\"NNLS R2 score\", r2_score_nnls)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "185b7e6b-044d-4973-9336-81c5a4a71122",
   "metadata": {},
   "source": [
    " Using the NNLS loss-function, we the $R^2$ of the modell\n",
    " $R^2$ is the measesure of how much variance of the model is explained by the model. Generally the higher the better. Our 0.82 is fine."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b97871f-fc3c-4925-8969-23f9ec8cbf9e",
   "metadata": {},
   "source": [
    "Next we plot the data and regression line to see, how it fits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61f4611b-5992-4d33-94a8-adcb720b99f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "ax.plot(reg_ols.coef_, reg_nnls.coef_, linewidth=0, marker=\".\")\n",
    "\n",
    "low_x, high_x = ax.get_xlim()\n",
    "low_y, high_y = ax.get_ylim()\n",
    "low = max(low_x, low_y)\n",
    "high = min(high_x, high_y)\n",
    "ax.plot([low, high], [low, high], ls=\"--\", c=\".3\", alpha=0.5)\n",
    "ax.set_xlabel(\"OLS regression coefficients\", fontweight=\"bold\")\n",
    "ax.set_ylabel(\"NNLS regression coefficients\", fontweight=\"bold\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
