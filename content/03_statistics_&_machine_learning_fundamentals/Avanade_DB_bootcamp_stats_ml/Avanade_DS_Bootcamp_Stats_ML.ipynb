{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "## <ins>Data Science Bootcamp - 3th Day- Statistics and ML</ins>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "In this chapter, we will take a look at the fundamentals of statistics needed for machine learning (from now on ML) and get an overview of the different kinds of machine learning. Due to the scope of topics, this will be a general overview. Later on in the bootcamp, there will be a more detailed look into the machine learning topics.<br>\n",
    "Most topics will be be accompanied by exertices on the topic.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Table of content:\n",
    "\n",
    "* [Data types](#ten)\n",
    "* [Probability theory](#one)\n",
    "* [Causality and correlation](#two)\n",
    "* [Moments](#three)\n",
    "* [Distributions](#four)\n",
    "* [Confidence intervals](#five)\n",
    "* [Hypothesis testing and signifinace tests](#six)\n",
    "* [Mathematical basics for ML](#seven)\n",
    "* [Types of ML](#eight)\n",
    "* [Deep learning](#nine)\n",
    "* [Learning](#ten)\n",
    "\n",
    "\n",
    "________________________________________________"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Statistics\n",
    "## Data types \n",
    "Data is information. But not all information is the same, data can fall into several different categories. <br>\n",
    "There are two types of data: Qualitative and Quantitative data, which are further classified into four types of data: nominal, ordinal, discrete, and continuous.<br>\n",
    "- nominal data is characterized by no clear order between the categories<br>\n",
    "- ordinal data is characterized by a clear distinction between the categories and have a order, but nothing in between them\n",
    "- discrete data is characterized by a clear distinction between the categories, moments like mean can be calulated.\n",
    "\n",
    "\n",
    "<img src=\"datatypes.jpg\" style=\"width: 350px;\" align=\"left\"/><br><br><br><br><br><br><br><br><br><br>\n",
    "Time series: <br>\n",
    "Time series are a series of data, that is indexed by a time scale.<br>Compared to non time dependent data, there are other phemomena and  have to be taken into account.<br>\n",
    "<img src=\"time_series.jpg\" style=\"width: 400px;\" align=\"left\"/>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "## Probability theory <a class=\"anchor\" id=\"one\"></a>\n",
    "1. Probabilities are described in numbers between 0 and 1.\n",
    ">  $P(X)=0.1$ describes the probability of an Event X happing to be 10%.<br><br>\n",
    "2. All possible events/probabilities put together have the probability of 1.<br>\n",
    "> The probiability of heads in a coin toss is 0.5, and for tails 0.5. <br>They are the only possible events and add up to 1.\n",
    "\n",
    "\n",
    "[Kolmogorov axioms](https://en.wikipedia.org/wiki/Probability_axioms)  \n",
    "___________________\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Probability density function\n",
    "Probabilities are displayed as a probability density function (PDF).<br>\n",
    "PDFs describe the distribution of all probabilities across the possible events.<br>\n",
    "<br> In this example, we have a [continous](https://en.wikipedia.org/wiki/Continuous_function) PDF.\n",
    "We can see, that function has it maximum at 0 with a probability of 0.4.\n",
    "\n",
    "<img src=\"pdf.png\" style=\"width: 300px;\" align=\"left\"/>\n",
    "<br><br><br><br><br><br><br><br><br>\n",
    "\n",
    "_________________________________\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cumulative distribution function\n",
    "Closely releated to the PDF is the cumulative distribution function (CDF).<br>\n",
    "\n",
    "Mathematically the CDF is the derivative of the PDF.<br>\n",
    "<img src=\"cdf.png\" style=\"width: 300px;\" align=\"left\"/><br><br><br><br><br><br><br><br><br>\n",
    "\n",
    "___________________________________"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Law of large numbers\n",
    "\n",
    "The law of large numbers is one of the most central laws in statistics.<br>\n",
    "It states, that in a series of random experiments the measured probability will over time converge on the expected probability.<br>\n",
    "<img src=\"Law_large_numbers.png\" style=\"width: 350px;\" align = \"left\"/>\n",
    "<br><br><br><br><br><br><br><br><br><br><br>\n",
    "$\\lim_{x\\to\\infty} \\sum_{i=1}^{n}\\frac{X_i}{n}=\\bar{X}$<br>\n",
    "As we will see later on ML, this is one of the reasons, why ML relies extensivly on repetions.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "[Law of large numbers](https://en.wikipedia.org/wiki/Law_of_large_numbers) \n",
    "___________________________________________"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Causality and correlation <a class=\"anchor\" id=\"two\"></a>\n",
    "Correlation is any statistical relationship, whether causal or not, between two random variables. \n",
    "The \n",
    "$corr(X,Y)=\\frac{E((X-\\mu_x))}{Y-\\mu_y)}\n",
    "\n",
    "\n",
    "\n",
    "[Correlation](https://en.wikipedia.org/wiki/Correlation)\n",
    "__________________________________\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Moments <a class=\"anchor\" id=\"three\"></a>\n",
    "\n",
    "Moments are measure, that describe the shape of a distribution. The four moments describe distributions  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$1^{th} moment: $ Mean or center of mass <br>$M_k=\\frac{1}{n}\\sum_{i=1}^n X_i^k$<br>\n",
    "Mean is one of the important and most commonly used measures\n",
    "\n",
    "<img src=\"mean.png\" style=\"width: 200px;\" align = \"left\"/>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$2^{th} moment: $ Variance <br>\n",
    "Variance is the expectation of the squared deviation of a random variable from its population mean or sample mean<br>\n",
    "$M_k=\\sum_{i=1}^n p_i \\cdot (x_i-\\mu)^2$<br>\n",
    "<img src=\"mean.png\" style=\"width: 200px;\" align = \"left\"/>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$3^{th} moment: $ Skewness <br>\n",
    "<img src=\"skewness.png\" style=\"width: 300px;\" align = \"left\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$4^{th} moment: $ Kurtosis <br>\n",
    "Kurtosis describes the \"pointyness\" of a distribution.<br>\n",
    "<img src=\"kurtosis.jpg\" style=\"width: 300px;\" align = \"left\"/>\n",
    "<br><br><br><br><br><br><br><br><br>\n",
    "___________________________________________________________\n",
    "[Moments](https://en.wikipedia.org/wiki/Moment_(mathematics))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quantils\n",
    "Quantiles are cut points dividing the range of a probability distribution into continuous intervals with equal probabilities, or dividing the observations in a sample in the same way.  <br> \n",
    "There are three groups of predefined quantile with their own name:<br>\n",
    "- Quartiles (four groups)<br> \n",
    "- Deciles (ten groups)<br>\n",
    "- Percentiles (100 groups)<br><br>\n",
    "<img src=\"quantile.png\" style=\"width: 350px;\" align = \"left\"/>\n",
    "<br><br><br><br><br><br><br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bias and Variance\n",
    "Bias is a systemic error in all measurements. This error is in all measurements and thus skews it away for the true value.\n",
    "Variance is the spread of the datapoints. <br>\n",
    "\n",
    "<img src=\"bias_variance.png\" style=\"width: 300px;\" align=\"left\"/>\n",
    "<br><br><br><br><br><br><br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "We will later on take a look at [Bias-Variance-Tradeoff](https://en.wikipedia.org/wiki/Bias%E2%80%93variance_tradeoff)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Distributions <a class=\"anchor\" id=\"four\"></a>\n",
    "\n",
    "Let's have a look at the most important distributions! <br>\n",
    "There are two different kinds of distrubtions: Discrete and continous\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Discrete:\n",
    "Binomial distribution <br>\n",
    "<img src=\"binomial.png\" style=\"width: 300px;\" align = \"left\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Continuous:\n",
    "$f(x)=\\frac{1}{b-a}$<br>\n",
    "<br><img src=\"uniform.png\" style=\"width: 300px;\" align = \"left\"/><br><br><br><br><br><br><br><br><br><br>\n",
    "[Uniform distribution](https://en.wikipedia.org/wiki/Continuous_uniform_distributionn)\n",
    "____________________________________\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "hide-cell"
    ]
   },
   "source": [
    " Normal distribution<br>\n",
    "$f(x)=\\frac{1}{\\sigma\\sqrt{2\\cdot \\pi}}\\cdot e^{-\\frac{1}{2}(\\frac{x-\\mu}{\\sigma})^2}$<br>\n",
    "<br><img src=\"normal.png\" style=\"width: 300px;\" align = \"left\"/><br><br><br><br><br><br><br><br><br><br>\n",
    "\n",
    "[Normal distribustion](https://en.wikipedia.org/wiki/Normal_distribution)\n",
    "________________________________"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "hide-cell"
    ]
   },
   "source": [
    "## Confidence intervals <a class=\"anchor\" id=\"five\"></a>\n",
    "Confidence intervalls are intervalls, where a certain percentage of values lie within.<br>\n",
    "For a confidence niveau of 95%, 95% of alle values will be within the CI.<br>\n",
    "Keep in mind, that each distribtution has a own formula for its confidence intervall.<br>\n",
    "<br>Confidence intervall for normal distribustion:<br><br>\n",
    "$[upper, lower] = \\bar{X}\\pm u\\cdot\\frac{\\sigma}{\\sqrt{n}}$<br>\n",
    "<img src=\"confidence-interval.jpg\" style=\"width: 350px;\" align = \"left\"/><br><br><br><br><br><br><br><br><br><br>\n",
    "Generally for more data the CI narrows down.\n",
    "\n",
    "\n",
    "The name for the probability of is [statistical significance.](https://en.wikipedia.org/wiki/Statistical_significance)\n",
    "\n",
    "\n",
    "\n",
    "[Confidence intervall](https://en.wikipedia.org/wiki/Confidence_interval)\n",
    "_____________________________________________________________________\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hypothesis testing and signifinace tests <a class=\"anchor\" id=\"six\"></a>\n",
    "Tests are done to check is a given obervation fits to a distribution e.g. normal distrubtion.<br>\n",
    "A test statistic is selected or defined in such a way as to quantify, within observed data, behaviours that would distinguish the null from the alternative hypothesis, where such an alternative is prescribed, or that would characterize the null hypothesis if there is no explicitly stated alternative hypothesis.\n",
    "\n",
    "Null hypothesis $H_0$ vs. alternative hyptohesis $H_1$\n",
    "\n",
    "One sided vs. two sided test:\n",
    "\n",
    "- two sided tests are for checking if a value is equal or different from a expected value.\n",
    "- one sided test are \n",
    "\n",
    "\n",
    "$H_0: \\mu = \\mu_0$ vs. $H_1: \\mu \\neq \\mu_0$\n",
    "\n",
    "\n",
    "### Normal distribution with two sides\n",
    "\n",
    "\n",
    "We are now using the t-Test <br>\n",
    "Reject $H_0 \\leftrightarrow T=\\sqrt{n}\\cdot\\frac{\\bar{X}-\\mu_0}{\\sigma}>t_{n-1,1-\\alpha} $<br>\n",
    "If this equition is meet the null hypothesis can be rejected and the mean of the obervation is different from expected mean.\n",
    "\n",
    "[Hypothesis testing](https://en.wikipedia.org/wiki/Statistical_hypothesis_testing)\n",
    "______________________________\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  <ins> Maschine Learning Fundamentals</ins>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ML definition\n",
    "> ML: The field of study that gives computers the ability to learn without being explicitly programmed. Arthur Samuel (1959).\n",
    "<br>\n",
    "\n",
    "ML-methods are algorithems, that take data and an algorithem and produces an function, that can be used to make new predictions.\n",
    "As such these methods are different from purely mathematical models, where a fixed formula is given and <br><br> \n",
    "\n",
    "<img src=\"ai_ml_dl.png\" style=\"width: 300px;\" align=\"left\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"ml_def.png\" style=\"width: 300px;\" align=\"left\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Applied ML\n",
    "Python is right now the most used language in data science and ML. On a survey on Kaggle, 83% of Data Scientists are regularly using Python <br>\n",
    "<img src=\"ml_language.svg\" style=\"width: 500px;\" align=\"left\"/>\n",
    "<br><br><br><br><br><br><br><br><br><br><br><br><br><br>\n",
    "The most important data science libaries used are:\n",
    "<ul>\n",
    "  <li>Matplotlib</li>\n",
    "  <li>Numpy</li>\n",
    "  <li>Scikit-learn</li>\n",
    "  <li>PyTorch</li>\n",
    "\n",
    "  <li>Keras</li>\n",
    "  <li>Pandas</li>\n",
    "</ul>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<img src=\"svl_usvl_rl.png\" style=\"width: 500px;\" align=\"left\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mathematical basics for ML <a class=\"anchor\" id=\"seven\"></a>\n",
    "### Matrix:\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The mathematical fundamentals of ML are matrixes and arithmetic operations are the . Matrix multiplication is resource intensive and slow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This table can be turned into a matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\begin{bmatrix}\n",
    "0.479 \\\\ 0.476 \\\\ 0.470 \\\\ 0.463 \\\\ 0.454 \n",
    "&\n",
    "65.0 \\\\ 59.9 \\\\ 59.9 \\\\ 59.5 \\\\ 59.2 \n",
    "\\end{bmatrix}$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Norms:\n",
    "Part of ML is measuring the difference between data points. Two norms are commonly used:\n",
    "Norms are used to measure those differences.<br>\n",
    "L1-Norm:<br>$\\Vert(x)\\Vert_1$<br>\n",
    "<img src=\"L1_norm.jpg\" style=\"width: 200px;\" align=\"left\"/><br><br><br><br><br><br><br><br><br>\n",
    "L2-Norm:<br>$\\Vert(x)\\Vert_2$<br>\n",
    "<img src=\"L2_norm.jpg\" style=\"width: 200px;\" align=\"left\"/><br><br><br><br><br><br><br><br><br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bias-Variance Tradeoff\n",
    "Bias-Variance Tradeoff is one of the central problens in ML.<br>\n",
    "The bias–variance dilemma or bias–variance problem is the conflict in trying to simultaneously minimize these two sources of error that prevent supervised learning algorithms from generalizing. <br>\n",
    "<img src=\"overfit_underfit.png\" style=\"width: 400px;\" align=\"left\"/><br><br><br><br><br><br><br>\n",
    "This dilemma leads to the below  tradeoff, we are trying to get to the sweetspot in witch the error of bias and variance is the lowest.<br><br>\n",
    "<img src=\"bias_variance_tradeoff.png\" style=\"width: 300px;\" align=\"left\"/><br><br><br><br><br><br><br><br><br><br><br>\n",
    "In order to check the validity and fitness of the model, data is usually split into 2 categories: <br>\n",
    "- training data on which the model is trained on \n",
    "- test data, on which the model is tested on.\n",
    "A split of 80:20 training/test is usually done.\n",
    "\n",
    "<img src=\"training_test.jpg\" style=\"width: 350px;\" align=\"left\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Types of ML <a class=\"anchor\" id=\"eight\"></a>\n",
    "### Supervised learning\n",
    "Supervised learning is the machine learning task of learning a function that maps an input to an output based on example input-output pairs.<br><br>\n",
    "<img src=\"supervised.png\" style=\"width: 450px;\" align=\"left\"/>\n",
    "<br><br><br><br><br><br><br><br><br><br><br>\n",
    "\n",
    "\n",
    "[Supervised learning](https://en.wikipedia.org/wiki/Supervised_learning)\n",
    "_______________________\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example: Linear regression\n",
    "\n",
    "In order to get a feeling for maschine learning, lets have a look at the most basic form of ML: <br> Linear regression<br>\n",
    ">$y(x)=\\beta+x\\cdot\\epsilon$\n",
    "\n",
    "Linear regression is a algorthim, that estimates the $\\beta$ and $\\epsilon$ of the above equation for a given dataset.<br><br>\n",
    "<img src=\"Normdist_regression.png\" style=\"width: 350px;\" align=\"left\"/><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br>\n",
    "Linear regression is the go to option for a lot of statistical analysis. One of the advantages is it has no bias in it. But it has prerequisites:\n",
    "- Linear relationship\n",
    "- No auto-correlation\n",
    "- Homoscedasticity\n",
    "If those conditions are not meat other methods or transformations needed to be taken.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loss-function\n",
    "Loss-function are a fundamental part of ML. They measure the difference between the predicted value and the expected value. \n",
    "There are different functions, that weight different aspect of the difference:<br>\n",
    "$MSE = \\frac{1}{n}\\cdot \\sum_{i=1}^n (Y_i-Y)^2$.<br>\n",
    "Mean squared error\n",
    "The mean squadared error penalizes values further away from the expected value.<br>\n",
    "There are several different loss functions, each better suited for a different purpose.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "[Linear regression](https://en.wikipedia.org/wiki/Linear_regression)<br>[Loss-function](https://en.wikipedia.org/wiki/Loss_function)\n",
    "__________________________________________________________"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Unsupervised learning\n",
    "Unsupervied learning is a categorgy of algorithems, that learns from unlabeled data.<br>\n",
    "Cluster analysis or clustering is the task of grouping a set of objects in such a way that objects in the same group (called a cluster) are more similar (in some sense) to each other than to those in other groups (clusters).<br>\n",
    "Generlly norms are used to measure the diferences between the data points.\n",
    "\n",
    "<img src=\"kmeans.png\" style=\"width: 300px;\" align=\"left\"/>\n",
    "<br><br><br><br><br><br><br><br>\n",
    "<ul>\n",
    "  <li>Affinity Propagation</li>\n",
    "  <li>Agglomerative Clustering</li>\n",
    "  <li>BIRCH</li>\n",
    "  <li>DBSCANh</li>\n",
    "  <li>K-Means</li>\n",
    "  <li>Mini-Batch K-Means</li>\n",
    "  <li>Mean Shift</li>\n",
    "  <li>OPTICS</li>\n",
    "  <li>Spectral Clustering</li>\n",
    "  <li>Gaussian Mixture Model</li>\n",
    "</ul>\n",
    "\n",
    "Hierarchical clustering (also called hierarchical cluster analysis or HCA) is a method of cluster analysis which seeks to build a hierarchy of clusters. Strategies for hierarchical clustering generally fall into two types:\n",
    "\n",
    "Agglomerative: This is a \"bottom-up\" approach: each observation starts in its own cluster, and pairs of clusters are merged as one moves up the hierarchy.<br>\n",
    "Divisive: This is a \"top-down\" approach: all observations start in one cluster, and splits are performed recursively as one moves down the hierarchy.<br><br>\n",
    "<img src=\"Hirachical.png\" style=\"width: 300px;\" align=\"left\"/><br><br><br><br><br><br><br><br><br><br><br>\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "[Unsupervised learning](https://en.wikipedia.org/wiki/Unsupervised_learning)\n",
    "____________________________________"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reinforcement learning\n",
    "\n",
    "\n",
    "Apart from supervised and unsupervised ML there is reinforcement learning which is an explorative machine learning algorithm where an agent learns strategies to maximize its rewards by interacting with an environment which either penalizes or rewards every action of the agent. Typically, the aim is to exploit by exporing new states which requires strategies for trade-offs between exploitation and exploration for which different algorithms exist (Q-learning, SARSA). RL can be applied in scenarios where a lot of data is available, e.g. in gameplay and robotics where simulations are readily available. \n",
    "\n",
    "Some key concepts describing a RL problem: \n",
    "\n",
    "- Agent: wants to maximize its reward through exploration of the environment \n",
    "- Environment: Physical world in which the agent operates\n",
    "- State: Current situation of the agent\n",
    "- Reward: Feedback from the environment\n",
    "- Policy: Method to map agent’s state to actions\n",
    "- Value: Future reward that an agent would receive by taking an action in a particular state\n",
    "\n",
    "\n",
    "<img src=\"reinforcement_learning.jpg\" style=\"width: 300px;\" align=\"left\"/>\n",
    "\n",
    "\n",
    "____________________________________________\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deep learning <a class=\"anchor\" id=\"nine\"></a>\n",
    "### Multilayer perceptron\n",
    "\n",
    "Deep Learning is a powerful division of Machine Learning where artifical neural networks (ANN) are at the heart of the learning algorithms inspired by the way neurons works in our brain. An ANN consists of several layers where each layer is made up of nodes with weighted connections to other nodes from different layers. Simply put, an ANN can be understood as a function with parameters which maps an input vector to an output vector or scalar. \n",
    "\n",
    "Here are the main components of the ANN: \n",
    "\n",
    "- __input layer__: fed with the different dimensions/features of the training data, which is then passed on and transformed through the network through to the \n",
    "- __output layer__:  contains the final result from the function. \n",
    "- __hidden layer__: layers between input and output layer\n",
    "- __node__: each layer consists of nodes which are connected to nodes from other layers through connections and weights. At all nodes from the hidden layer and the output layer a dot product computation is performed between the output values ($x_i$) of the previous nodes and weights ($w_i$) attached to every connection. Then, an activation function is applied which maps the dot product either to 0 or 1 or depending on the scenario, to a probability value in the case of nodes from the ouput layer\n",
    "- __weight__: the weight is the strenght at which the training data is strenghthened to \n",
    "- __loss function__: as with every other ML algorithm, the goal is to learn the model parameters (*weights*) through multiple training iterations in which the error between the current outcome and the expected outcome is reduced\n",
    "- __activation functions__: \n",
    "\n",
    "\n",
    "\n",
    "<img src=\"mlp.png\" style=\"width: 300px;\" align=\"left\"/>\n",
    "\n",
    "\n",
    "#### Backpropogation\n",
    "\n",
    "\n",
    "The objective in trianing a neural network is is to reduce the error between the expected outcome and model outcome with the current parameters. The first iteration, typically starts with random parameter initializations which lead to some model outcome and the resulting error. The partial derivates of the error function with regard to each weight parameter are then used to adjust the weights such that the error is iteratively minimized, and a cat is recognized as a cat, \n",
    "Bropagation refers to the propagtion of errors through the layers and the utilization of the partial derivative of the gradient from a previous layer in the current layer. \n",
    "\n",
    "Illustratively, the objective is to find the local or global minimum of the error function. The gradient describes the steepness of the error function, so during the training process we want to go downhill from the initial random state of the weight parameters. \n",
    "<br><br>\n",
    "\n",
    "\n",
    " <img src=\"gradient_descent.jpeg\" style=\"width: 300px;\" align=\"left\"/>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Computer vision basics\n",
    "\n",
    "In computer vision, the Convoluted Neural Network (CNN) is commonly used for image recognition. \n",
    "\n",
    "A CNN takes as input an image of n x m pixels with 3 color channels RGB, resulting in a n x m x 3 image-vector. The output vector encodes the information whether it is a dog, a cat, a bird or whatever image objects the model is trained on. \n",
    "A CNN consists of 3 types of hidden layers: convolution, pooling and fully connected feed forward layer. \n",
    "\n",
    "<img src=\"cnn_filter.png\" style=\"width: 400px;\" align=\"left\"/>\n",
    "\n",
    "<br><br><br><br><br><br><br><br><br><br><br><br>\n",
    "\n",
    "In the convolution layer, a kernel (filter) is used to scan the image and extract features of the image by applying dot product operations and downsizing it. In the lower convolution layers kernels will extract low-level features like lines, edges, curves, in the upper levels features will be more sophisticated like the unique shapes, color and other characteristics of eyes, nose, lips etc. belonging to a specific person. \n",
    "<br><br>\n",
    "\n",
    "<img src=\"cnn_pooling.png\" style=\"width: 400px;\" align=\"left\"/>\n",
    "\n",
    "<br><br><br><br><br><br><br><br><br><br><br><br>\n",
    "\n",
    "A convolution layer is followed by a pooling layer which further reduces the amount of information and performs noise-cancellations by applying max or average pooling. Finally, a fully connected feed forward layer is applied where the image n x m x 3 dimensional image-vector is flattened and then mapped to the output vector containing the information on the image class. \n",
    "\n",
    "\n",
    "<img src=\"cnn_before_after.png\" style=\"width: 400px;\" align=\"left\"/>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Recurrent Neural Networks (RNN)\n",
    "\n",
    "RNNs are capable of processing sequence data. Sequence data is a series of input data where the order plays a role, e.g. in text pieces and time series data. So at every given point the output of the RNN as the next sequence item is dependent on not just the input vector but also a state which contains the context information based on all previous inputs and the resulting states. So the RNN has a got memory. The following image shows a representation of the RNN which can be unrolled in time: \n",
    "\n",
    "<img src=\"rnn.png\" style=\"width: 450px;\" align=\"left\"/><br><br><br><br><br><br><br><br>\n",
    "\n",
    "What it means is that at time t=0, the RNN takes X(0) and returns the hidden state h(0) which together with X(1) returns h(1). X(2) uses h(1) to generate h(2) and so on. This way the RNN remembers the states which influence the output of the model. \n",
    "\n",
    "[RNN](https://en.wikipedia.org/wiki/Recurrent_neural_network)\n",
    "[Autocorrelation](https://en.wikipedia.org/wiki/Autocorrelation)\n",
    "_________________________________  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Natural Language Processing (NLP)\n",
    "\n",
    "NLP is a field of AI and ML that enables machines to read, understand and derives meaning from human languages. Data generated from articles, conversations and social media are examples of unstructured text data which is messy and hard to manipulate. Thanks to advances in NLP and ML, there are various techniques that enable machines to go beyond the analysis of key words and understand context and semantic nuances behind a text, e.g.figures of speech like irony.\n",
    "Examples of Use cases for NLP are: \n",
    "- machine translation\n",
    "- sentiment analysis e.g. in social media or based on product reviews\n",
    "- cognitiive Assistants and Chatbots \n",
    "- Spam classification\n",
    "- identitifying fake news\n",
    "- diagnosis in health care through improved documentation of health records\n",
    "\n",
    "\n",
    "#### Bag of words\n",
    "Bag words represent occurrence matrices for a sentence or document, disregarding grammar or word order. They are used as features for training a classfier.\n",
    "\n",
    "#### Tokenization\n",
    "To to be able to process unstructured data, text is broken down into pieces of words and terms called tokens, often split by space. In this process certain characters, like punctuation is removed.The process of segmenting text into sentences and words is called tokenization. Each token then obtains a clear index which plays a role when retrieving the document which matches a sentence searched. Tokens are also the building blocks of input sequences fed into neural networks and Machine learning algorithms. \n",
    "\n",
    "#### Stemming and Lemmatization\n",
    "Stemming and Lemmatization aim at unifying words which have the same semantic root. This helps to reduce noise from grammar, spelling errors, plural forms. Stemming cuts the word bearing the risk that the meaning gets changed while Lemmatization neutralizes the grammatical tense, e.g. <br>\n",
    "__Stemming__: Cars -> Car <br>\n",
    "__Lemmatization__: Caring -> Care\n",
    "\n",
    "\n",
    "<br>\n",
    "\n",
    "\n",
    "\n",
    "<img src=\"token_nlp.jpg\" style=\"width: 400px;\" align=\"left\"/>\n",
    "<br><br><br><br><br><br><br>\n",
    "Vocabulary: <br>\n",
    "<img src=\"vocab_nlp.jpeg\" style=\"width: 400px;\" align=\"left\"/>\n",
    "<br><br><br><br><br><br><br><br><br><br><br><br>\n",
    "\n",
    "#### Word embbeding: <br>\n",
    "\n",
    "Words are represented as numerical vectors and through similarity scores relative meaning is learned using Machine Learning. \n",
    "\n",
    "\n",
    "<img src=\"word_embbedings.jpeg\" style=\"width: 400px;\" align=\"left\"/>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Learning <a class=\"anchor\" id=\"ten\"></a>\n",
    "[Kaggle](https://www.kaggle.com/): Datasets, Courses and Datascience challenges<br>\n",
    "[Towardsdatascience](https://towardsdatascience.com/): Blogs about Datascience"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Create Assignment",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "toc-autonumbering": true,
  "toc-showmarkdowntxt": true,
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
