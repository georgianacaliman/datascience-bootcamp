{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this assignment notebook, you will perform the preprocessing of the dataset \"data-science-bootcamp/content/datasets/classification/raw/classification_25.csv\". \n",
    "\n",
    "All the cells are filled with commented instructions and hints and partial codes. The parts that you are supposed to write your code is indicated with three dots \"...\".\n",
    "\n",
    "You can find the solution to the assignment in the notebook \"data-science-bootcamp/content/04_data_preprocessing_&_feature_engineering/Classification_dataset_preprocessing.ipynb\"\n",
    "\n",
    "The dataset is a modified version of \"default_of_credit_card_clients.xls\" from https://archive.ics.uci.edu/ml/datasets/default+of+credit+card+clients. The slight modifications are done for the pedagogical reasons of demonstrating different methods of preprocessing and feature engineering (particularly, missing value imputation and categorical encoding).\n",
    "This binary classification dataset contains 23 different attributes of the customers and 1 binary column \"default payment next month\" which is the target column to be predicted by the ML model. \n",
    "\n",
    "The notebook contains the following sections:\n",
    "\n",
    "0. Exploratory Data Analysis\n",
    "1. Missing Value Imputation\n",
    "1.0. Identifying columns with null values \\\n",
    "1.1. Complete Case Analysis\\\n",
    "1.2. Mean or Median Imputation \\\n",
    "1.3. Frequent Category Imputation (Mode Imputation)\n",
    "2. Categorical encoding\\\n",
    "2.1. One Hot Encoding \n",
    "3. Handling Outliers\\\n",
    "3.1. Identifying the outliers \\\n",
    "3.2. Outlier trimming \\\n",
    "3.3. Outlier Capping\n",
    "4. Feature Scaling\\\n",
    "4.1. Min-Max Scaling\n",
    "5. Feature Selection\\\n",
    "5.1. Dropping Constant Features \\\n",
    "5.2. Correlation-based Feature Selection\\\n",
    "5.2.a Selecting Features with hight correlations with Target\\\n",
    "5.2.a Selecting Features with low correlations with other features\n",
    "6. Saving the preprocessed data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 0. Exploratory Data Analysis (EDA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import the necessary libraries (pandas, numpy, matplotlib.pyplot, seaborn) \n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "...\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 0.1 Loading the data into a dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the \"../datasets/classification/raw/classification_25.csv\" into a dataframe called \"df\"\n",
    "# note that the delimiter is \";\"\n",
    "df = ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#display the first 5 rows of df to familiarize yourself with the data\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop the first column (\"ID\")\n",
    "# do not forget to set inplace= True\n",
    "..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check the shape of the dataframe as a 2-dimensional data object (#rows, #columns)\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataset contains 30000 records (rows) and 24 columns (features + target):\n",
    "\n",
    "The target column (y) is called \"default payment next month\".\n",
    "The rest 23 columns (X) are our feature vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# store the \"default payment next month\" in a variable called \"target\", and the rest of columns' names in a variable called \"X\"\n",
    "target = \"default payment next month\"\n",
    "X = list(set(df.columns) - set([target])) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the distribution of the classes in the target variable\n",
    "..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate the imbalance ration of the target classes\n",
    "class_0 = df[target].value_counts().to_list()[0]\n",
    "class_1 = df[target].value_counts().to_list()[1]\n",
    "original_imbalance_ratio = class_1/class_0\n",
    "print(\"Imbalance ratio =\", original_imbalance_ratio)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The imbalance ratio is 0.28 which is good enough for this classification problem, given the size of the dataset (30000)\\\n",
    "we will later compare the original_imbalance_ratio with the train_imbalance_ration which is obtained after train/test split and demand that they are not very different."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We first have a look at the data types of all columns\n",
    "..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# classifying feature columns (X) with different data types:\n",
    "num_cols = []\n",
    "for col, dtype in df[X].dtypes.to_dict().items():\n",
    "    if str(dtype) in 'int64' or str(dtype) in 'float64':\n",
    "        num_cols.append(col)\n",
    "print(\"Number of numerical Columns:\", len(num_cols))\n",
    "\n",
    "str_cols = ...\n",
    "\n",
    "print(\"Number of String Columns:\", len(str_cols))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Missing Value Imputation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.0. Identifying columns with null values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a list, called \"null_cols\", of all columns with null values\n",
    "null_cols = ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(null_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Number of Columns with Null values:\", len(null_cols))\n",
    "print(\"Percentage of null values:\")\n",
    "for col in null_cols:\n",
    "    print(col, round(df[col].isnull().mean(), 4), \", Data Type: \", df[col].dtype)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We observe that \"EDUCATION\" and \"AGE\" have less than 2% missing values and \"MARRIAGE\" has 0.18% missing value.\\\n",
    "For the column \"MARRIAGE\", we perform Complete Case Analysis,\\\n",
    "For the integer column \"AGE\", we perform mean/median imputation, \\\n",
    "For the categorical column \"EDUCATION\", we perform Frequent Category Imputation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1. Complete Case Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for the column \"MARRIAGE\" drop all records which contain a null value\n",
    "df = ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We observe that after dropping the missing values, the number of records is reduced from 30000 to 29946."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2. Mean or Median Imputation: fillna()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before filling the null values, we split the data into train and test sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(df, df[target], test_size=0.3, random_state=10)\n",
    "\n",
    "X_train.shape, X_test.shape, y_train.shape, y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_class_0 = y_train.value_counts().to_list()[0]\n",
    "y_train_class_1 = y_train.value_counts().to_list()[1]\n",
    "train_imbalance_ration = y_train_class_1/y_train_class_0\n",
    "print(\"Original imbalance ratio =\", original_imbalance_ratio)\n",
    "print(\"Imbalance ratio after train/test split =\", train_imbalance_ration)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We observe that train/test splitting procedure has a very tiny influence on the imbalance ration, which is what we want."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before deciding whether we implement the mean or median imputation, we plot the distribution of \"AGE\" "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the distribution of df['AGE'] \n",
    "..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Mean of AGE: \", df['AGE'].mean())\n",
    "print(\"Median of AGE: \", df['AGE'].median())\n",
    "print(\"Mode of AGE: \", df['AGE'].mode())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the data distribution is skewed, we perform median imputation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# store the median of X_train['AGE'] in a new variable called \"AGE_median\" \n",
    "AGE_median = ...\n",
    "# fill the missing values of X_train['AGE'] with AGE_median in a new column called \"X_train['AGE_median']\"\n",
    "X_train['AGE_median'] = ...\n",
    "# fill the missing values of X_test['AGE'] with AGE_median in a new column called \"X_test['AGE_median']\"\n",
    "X_test['AGE_median'] = ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the distribution of X_train['AGE'] and X_train['AGE_median'] with two different colors in one diagram\n",
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We observe that the distribution of the \"AGE\" column is not distorted by meadian imputation, as desired. Therefore, we keep the new column 'AGE_median' and drop the old column \"AGE\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop the column \"AGE\" from X_train and X_test\n",
    "X_train.drop(columns=[\"AGE\"], axis=1, inplace=True)\n",
    "X_test.drop(columns=[\"AGE\"], axis=1, inplace=True)\n",
    "\n",
    "X_train.shape, X_test.shape, y_train.shape, y_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3. Frequent Category Imputation (Mode Imputation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the distribution of values in X_train['EDUCATION']\n",
    "..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# store the mode of X_train[\"EDUCATION\"] in a variable called \"EDUCATION_MODE\"\n",
    "EDUCATION_MODE = ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The most frequent catregories in X_train['EDUCATION'] is the \"university\" category."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# implement the mode imputation for X_train['EDUCATION'] and X_test['EDUCATION']\n",
    "..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.shape, X_test.shape, y_train.shape, y_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Categorical encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Recall that we have 3 categorical columns\n",
    "print(str_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print the distinct catgories in the categorical columns\n",
    "for col in str_cols:\n",
    "    print(col, X_train[col].unique())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1. One Hot Encoding (using Scikit-Learn):\n",
    "We encode the three categorical features using the OHE."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.shape, X_test.shape, y_train.shape, y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import the OneHotEncoder from sklearn.preprocessing\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "# create an encoder as an instance of OneHotEncoder with parameters sparse= False, handle_unknown='ignore'\n",
    "encoder = ...\n",
    "# fit the encoder to the X_train[str_cols]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# transform the categorical columns in X_train\n",
    "X_train_transformed = encoder.transform(X_train[str_cols])\n",
    "# turn the transformed Numpy array into a Pandas dataframe\n",
    "X_train_OHE_skl = pd.DataFrame(X_train_transformed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_OHE_skl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# as we can see, the new encoded columns are named as integers. \n",
    "# However, the names of the features are stored in the get_feature_names() method\n",
    "encoder.get_feature_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a dummy_cols_list which contains the names of the categorical columns and their category in the format \"col_cat\"\n",
    "dummy_cols_list = list(encoder.get_feature_names())\n",
    "for i in range(0, len(dummy_cols_list)):\n",
    "    for j in range(0, len(str_cols)):\n",
    "        dummy_cols_list[i] = dummy_cols_list[i].replace(f\"x{j}_\", str_cols[j] + \"_\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we rename the dataframe columns to the above names stored in dummy_cols_list\n",
    "X_train_OHE_skl.columns = ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_OHE_skl.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# transform the categorical columns in X_test\n",
    "X_test_transformed = ...\n",
    "# turn the transformed Numpy array into a Pandas dataframe\n",
    "X_test_OHE_skl = ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rename the new encoded columns using dummy_cols_list\n",
    "X_test_OHE_skl.columns = ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_OHE_skl.shape, X_test_OHE_skl.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now concatinate the original X_train/X_test and the encoded X_train_OHE_skl/X_test_OHE_skl (and then drop the categorical columns from the resulting dataframe). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = pd.concat([X_train.reset_index(drop=True), X_train_OHE_skl.reset_index(drop=True)], axis=1)\n",
    "X_train = X_train.drop(columns=str_cols, axis=1)\n",
    "X_test = pd.concat([X_test.reset_index(drop=True), X_test_OHE_skl.reset_index(drop=True)], axis=1)\n",
    "X_test = X_test.drop(columns=str_cols, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.shape, X_test.shape, y_train.shape, y_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Handling Outliers\n",
    "3.1. Identifying the outliers \\\n",
    "3.2. Outlier trimming \\\n",
    "3.3. Outlier Capping\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1. Identifying the outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the box-plot of feature columns (it is sufficient to plot only a couple of them ['PAY_AMT2', 'BILL_AMT1', 'BILL_AMT3', 'PAY_AMT1', 'PAY_AMT3'])\n",
    "plt.rcParams[\"figure.figsize\"] = (50,10)\n",
    "fig, axs = plt.subplots(ncols=5, constrained_layout=True)\n",
    "sns.boxplot(y=..., ax=axs[0])\n",
    "sns.boxplot(y=..., ax=axs[1])\n",
    "sns.boxplot(y=..., ax=axs[2])\n",
    "sns.boxplot(y=..., ax=axs[3])\n",
    "sns.boxplot(y=..., ax=axs[4])\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate the IQR, lower_boundary, upper_boundary and outlier_list of all columns.\n",
    "# create the following four dictionaries and store the relevant data in them\n",
    "IQR_dict= {}\n",
    "for col in X_train.columns:\n",
    "    IQR_dict[f\"{col}_IQR\"] = ...\n",
    "\n",
    "lower_boundary_dict= {}\n",
    "for col in X_train.columns:\n",
    "    lower_boundary_dict[f\"{col}_lower_boundary\"] = ...\n",
    "\n",
    "upper_boundary_dict= {}\n",
    "for col in X_train.columns:\n",
    "    upper_boundary_dict[f\"{col}_upper_boundary\"] = ...\n",
    "\n",
    "columns_outliers_dict={}\n",
    "for col in X_train.columns:\n",
    "    columns_outliers_dict[f\"{col}_outliers_list\"] = ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\" >>IQR_dict: \", IQR_dict)\n",
    "print(\" >>lower_boundary_dict: \", lower_boundary_dict)\n",
    "print(\" >>upper_boundary_dict: \", upper_boundary_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creatre a list, called \"outlier_cols\", containing all columns' names which have more than 0% outliers.\n",
    "outlier_cols =...\n",
    "print(outlier_cols)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2. Outlier trimming\n",
    "Drop all the records which include an outlier (in X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.shape, y_train.shape, X_test.shape, y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create a list, called \"outlier_10percent_cols\", of columns with less than 10% outliers\n",
    "outlier_10percent_cols = [col for col in outlier_cols if columns_outliers_dict[f\"{col}_outliers_list\"].mean() < 0.1 ] \n",
    "\n",
    "print(outlier_10percent_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop all the records of the outlier_10percent_cols which contain outliers in X_train and y_train\n",
    "X_train = X_train.loc[~(\n",
    "+ columns_outliers_dict[...]\n",
    "+ columns_outliers_dict[...]\n",
    "), \n",
    "]\n",
    "\n",
    "y_train = y_train.loc[~(\n",
    "+ columns_outliers_dict[...]\n",
    "+ columns_outliers_dict[...]\n",
    "), \n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.shape , X_test.shape, y_train.shape, y_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The outlier trimming procedure has reduced the 20962 records in X_train/y_train to 20659 records.\\\n",
    "We also need to check if the Imbalance ration is changed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_class_0 = y_train.value_counts().to_list()[0]\n",
    "y_train_class_1 = y_train.value_counts().to_list()[1]\n",
    "print(\"Imbalance ratio X_train after outlier trimming =\", y_train_class_1/y_train_class_0)\n",
    "print(\"Original imbalance ratio =\", original_imbalance_ratio)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We observe that the outlier trimming procedure has a very tiny influence on the imbalance ration, which is what we desired."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3. Outlier Capping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a set of the columns with more than 10 percent outliers\n",
    "outlier_90percent_cols = ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(outlier_90percent_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# implement the outlier capping method for outlier_90percent_cols\n",
    "for col in outlier_90percent_cols:\n",
    "    X_train[col] = ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.shape , X_test.shape, y_train.shape, y_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We observe that outlier capping does not reduce the dataset size."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Feature Scaling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1. Min-Max Scaling\n",
    "we perform min-max scaling using sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import MinMaxScaler from sklearn.preprocessing\n",
    "...\n",
    "# create a scaler as an instance of MinMaxScaler\n",
    "scaler = ...\n",
    "# fit X_train to the scaler\n",
    "...\n",
    "# transform X_train and X_test using the fitted scaler\n",
    "X_train_scaled = ...\n",
    "X_test_scaled = ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Feature Selection\n",
    "5.1. Dropping Constant Features \\\n",
    "5.2. Correlation-based Feature Selection\\\n",
    "5.2.a Selecting Features with low correlations with other features\\\n",
    "5.2.a Selecting Features with hight correlations with Target"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.1. Dropping Constant Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a list of constant features in X_train (i.e. those whose number of unique values is 1)\n",
    "const_feat = ...\n",
    "print(const_feat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop the constant features from X_train and X_test\n",
    "...\n",
    "..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.shape , X_test.shape, y_train.shape, y_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2. Correlation-based Feature Selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2.a Selecting Features with hight correlations with Target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We first add the target (y_train) to the Features (X_train) to be able to calculate the correlation matrix between them\n",
    "df_train = pd.concat([X_train.reset_index(drop=True), y_train.reset_index(drop=True)], axis = 1)\n",
    "# create the correlation matrix\n",
    "cor_mat = ...\n",
    "# Add a column which contains the sorted absolute values of corr(X_train, y_train)\n",
    "cor_mat[\"X-Y_corr\"]= ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cor_mat[\"X-Y_corr\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# collect all the features with corr(X_train, y_train) > 0.014 into a list\n",
    "final_cols = cor_mat[cor_mat[\"X-Y_corr\"] > 0.014].index.values.tolist()\n",
    "final_cols = list(set(final_cols) - set([target]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(final_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# replace the columns in X_train and X_test with final_cols\n",
    "X_train = ...\n",
    "X_test = ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.shape, y_train.shape, X_test.shape, y_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2.b Selecting Features with low correlations with other features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the correlation diagram of X_train\n",
    "..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a set of column pairs whose correlation is higher than 88%\n",
    "col_corr = set()\n",
    "corr_matrix = X_train.corr()\n",
    "\n",
    "for i in range(len(corr_matrix.columns)):\n",
    "    for j in range(i):\n",
    "        if abs(corr_matrix.iloc[i, j]) > 0.9: \n",
    "            print(abs(corr_matrix.iloc[i, j]), corr_matrix.columns[i], corr_matrix.columns[j])\n",
    "            colname = corr_matrix.columns[j]\n",
    "            col_corr.add(colname)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dropping one of the columns in each pair in col_corr in X_train and X_test\n",
    "...\n",
    "..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.shape, y_train.shape, X_test.shape, y_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. Saving the preprocessed data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the preprocessed data (X_train, y_train, X_test, y_test) as four csv files with \",\" as delimiter, and with \"utf-8\" encoding to the folder \"../datasets/classification/processed\"\n",
    "X_train.to_csv(...)\n",
    "X_test.to_csv(...)\n",
    "y_train.to_csv(...)\n",
    "y_test.to_csv(...)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "e737f341e96eee98a3a2acbd8b0c12a6221c684ebe624d9d044b4a2a2a65610f"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
